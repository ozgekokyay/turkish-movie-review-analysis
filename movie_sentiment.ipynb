{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie-sentiment.ipynb adlı not defterinin kopyası",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozgekokyay/turkish_movie_review_analysis/blob/main/movie_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivxgx5bZuRyV"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from tensorflow.keras.models import Sequential, load_model\r\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding, Bidirectional\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import tensorflow as tf\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4t5E-pL-xpL"
      },
      "source": [
        "!wget https://www.dropbox.com/s/fz2d3s2ngq8aw2e/turkish_movie_sentiment_dataset.csv?dl=1 -O dataset.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBH34B9fBvkt"
      },
      "source": [
        "data = pd.read_csv('dataset.csv')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QkDk65FaCbV"
      },
      "source": [
        "def convert(point):\r\n",
        "  p = point.split(',')\r\n",
        "  a = int(p[0])*10 + int(p[1])\r\n",
        "  if a > 30:\r\n",
        "    return 1\r\n",
        "  else:\r\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctI5_bm0sm7P"
      },
      "source": [
        "data_point =data['point'].apply(convert)\r\n",
        "target = data_point.values.tolist()\r\n",
        "\r\n",
        "comment = data['comment'].values.tolist()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YScJhjU-tLm1"
      },
      "source": [
        "cutoff = int(len(comment)*0.8)\r\n",
        "x_train, x_test = comment[:cutoff], comment[cutoff:]\r\n",
        "y_train, y_test = target[:cutoff], target[cutoff:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK3giHZGt9Em"
      },
      "source": [
        "num_words=10000\r\n",
        "tokenizer = Tokenizer(num_words=num_words)\r\n",
        "tokenizer.fit_on_texts(comment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbn_2gS3uaKy"
      },
      "source": [
        "x_train_tokens = tokenizer.texts_to_sequences(x_train)\r\n",
        "x_test_tokens = tokenizer.texts_to_sequences(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1ZB7h2VxRyM"
      },
      "source": [
        "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\r\n",
        "num_tokens = np.array(num_tokens)\r\n",
        "np.mean(num_tokens), np.max(num_tokens), np.argmax(num_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPjirpN4y3zn"
      },
      "source": [
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\r\n",
        "max_tokens = int(max_tokens)\r\n",
        "max_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-JqwQhV0QUz"
      },
      "source": [
        "np.sum(num_tokens<max_tokens)/len(num_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv6kO7Qe0gmb"
      },
      "source": [
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\r\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)\r\n",
        "x_train_pad.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvJbVLLD12_-"
      },
      "source": [
        "idx = tokenizer.word_index\r\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8idcYd4N2I2U"
      },
      "source": [
        "def tokens_to_string(tokens):\r\n",
        "  words =[inverse_map[token] for token in tokens if token!=0]\r\n",
        "  text =' '.join(words)\r\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9U6v71H6ItV"
      },
      "source": [
        "from tensorflow.keras.layers import LSTM, Dropout\r\n",
        "model = tf.keras.Sequential()\r\n",
        "embedding_size = 65\r\n",
        "model.add(Embedding(input_dim=num_words,\r\n",
        "                    output_dim=embedding_size,\r\n",
        "                    input_length=max_tokens,\r\n",
        "                    name='embedding_layer'))\r\n",
        "\r\n",
        "model.add(Dense(64, activation='sigmoid'))\r\n",
        "model.add(GRU(units=32, return_sequences=True))\r\n",
        "model.add(GRU(units=16, return_sequences=True))\r\n",
        "model.add(GRU(units=8, return_sequences=True))\r\n",
        "model.add(GRU(units=4))\r\n",
        "model.add(Dense(4, activation='tanh'))\r\n",
        "\r\n",
        "model.add(Dense(1))\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CysymIvw7v8t"
      },
      "source": [
        "optimizer = Adam(lr=1e-3)\r\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=optimizer, metrics=['accuracy'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI4VtyzJACRM"
      },
      "source": [
        "from tensorflow.data import Dataset\r\n",
        "train_ds = Dataset.zip((Dataset.from_tensor_slices(x_train_pad), Dataset.from_tensor_slices(y_train)))\r\n",
        "train_ds = train_ds.shuffle(1024).batch(256)\r\n",
        "val_ds = Dataset.zip((Dataset.from_tensor_slices(x_test_pad), Dataset.from_tensor_slices(y_test)))\r\n",
        "val_ds = val_ds.batch(256)\r\n",
        "steps_per_epoch = len(x_train_pad)//256\r\n",
        "validation_steps = len(x_test_pad)//256\r\n",
        "model.fit(train_ds.repeat(),  epochs=4, steps_per_epoch=steps_per_epoch, validation_data=val_ds.repeat(), validation_steps=validation_steps)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}